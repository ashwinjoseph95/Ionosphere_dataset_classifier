{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ooGKfHnfQe1P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "import csv\n",
        "from matplotlib import pyplot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading iris 2 class dataset\n",
        "iris=0\n",
        "ionosphere=1\n",
        "if iris:\n",
        "  data = np.genfromtxt('iris_2class.data', delimiter = ',')\n",
        "  #extracting first 4 columns for input features\n",
        "  X = data[:,:4]\n",
        "  #extracting last for label\n",
        "  y = data[:, 4]\n",
        "elif ionosphere:\n",
        "  \n",
        "  \n",
        "  data_filename='ionosphere.data'\n",
        "  \n",
        "  # Size taken from the dataset and is known\n",
        "  X = np.zeros((351, 34), dtype='float')\n",
        "  y = np.zeros((351,), dtype='bool')\n",
        "\n",
        "  with open(data_filename, 'r') as input_file:\n",
        "      reader = csv.reader(input_file)\n",
        "      for i, row in enumerate(reader):\n",
        "          # Get the data, converting each item to a float\n",
        "          data = [float(datum) for datum in row[:-1]]\n",
        "          # Set the appropriate row in our dataset\n",
        "          X[i] = data\n",
        "          # 1 if the class is 'g', 0 otherwise\n",
        "          if row[-1] == 'g':\n",
        "            y[i]=1\n",
        "          else:\n",
        "            y[i]=0\n",
        "\n",
        "\n",
        "# print(data.shape)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "# print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nhm27IEjU3d6",
        "outputId": "e670e589-02c0-4a15-8f6c-5fce1d25603b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(351, 34)\n",
            "(351,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting dataset in 4:1 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = X_train.T\n",
        "y_train = y_train.reshape(1, y_train.shape[0])\n",
        "X_test = X_test.T\n",
        "y_test = y_test.reshape(1, y_test.shape[0])\n",
        "\n",
        "\n",
        "print ('Total Training Samples: {}'.format (X_train.shape[1]))\n",
        "print ('Train X Shape: ', X_train.shape)\n",
        "print ('Train Y Shape: ', y_train.shape)\n",
        "print ('\\nTest X Shape: ', X_test.shape)\n",
        "print ('y_test Shape: ', y_test.shape)\n",
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ha_SFfNRSrs",
        "outputId": "92ba243f-6500-4373-aa25-3d89af4e20dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Training Samples: 280\n",
            "Train X Shape:  (34, 280)\n",
            "Train Y Shape:  (1, 280)\n",
            "\n",
            "Test X Shape:  (34, 71)\n",
            "y_test Shape:  (1, 71)\n",
            "[[ 1.       1.       1.      ...  1.       1.       0.     ]\n",
            " [ 0.       0.       0.      ...  0.       0.       0.     ]\n",
            " [ 0.4709   0.82254  0.89589 ...  0.79847  1.       1.     ]\n",
            " ...\n",
            " [-0.24339 -0.0422  -0.07029 ... -0.1875   0.00246  1.     ]\n",
            " [ 0.2672   0.78439  0.76862 ...  1.       0.17758  1.     ]\n",
            " [ 0.04233  0.01214  0.27926 ... -0.0574   0.7979   1.     ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define the number if neurons in input layer, hidden layers (taking as 4) and output layer\n",
        "input_unit = X.shape[1] # size of input layer\n",
        "hidden_unit = 34 #hidden layer of size 4\n",
        "output_unit = 1 # size of output layer\n",
        "\n",
        "print(\"The size of the input layer is:  = \" + str(input_unit))\n",
        "print(\"The size of the hidden layer is:  = \" + str(hidden_unit))\n",
        "print(\"The size of the output layer is:  = \" + str(output_unit))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqU8LJTZRWs0",
        "outputId": "a563bff4-b486-4b93-ec17-ea321ff8abb5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the input layer is:  = 34\n",
            "The size of the hidden layer is:  = 34\n",
            "The size of the output layer is:  = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parameters_initialization(input_unit, hidden_unit, output_unit):\n",
        "    #\"Initialize the parameters for a two-layer network and for an  L -layer neural network\"\n",
        "    \n",
        "    \n",
        "    np.random.seed(2) \n",
        "    #\"COMPLETE CODE SNIPPET BELOW\"\n",
        "    W1 = np.random.randn(hidden_unit, input_unit)*0.01\n",
        "    b1 = np.zeros((hidden_unit, 1))\n",
        "    W2 = np.random.randn(output_unit, hidden_unit)*0.01\n",
        "    b2 = np.zeros((output_unit, 1))\n",
        "\n",
        "    assert(W1.shape == (hidden_unit, input_unit))\n",
        "    assert(b1.shape == (hidden_unit, 1))\n",
        "    assert(W2.shape == (output_unit, hidden_unit))\n",
        "    assert(b2.shape == (output_unit, 1))\n",
        "\n",
        "    #assigning parameter values to dictionary\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "metadata": {
        "id": "D7inFE2l4sex"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    #Implementing the sigmoid activation function\n",
        "\n",
        "    #Fill the Code snippet Below\n",
        "    act_out = 1/(1+np.exp(-z))\n",
        "\n",
        "    return act_out\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    #Implement Forward propagation\n",
        "\n",
        "    #Extracting parameter values from dictionary\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    #Complete Code Snippet below\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    #Till here\n",
        "\n",
        "    #Storing each individual matrics computed for assisting with Back prop\n",
        "    cache = {\"Z1\": Z1,\"A1\": A1,\"Z2\": Z2,\"A2\": A2}\n",
        "    \n",
        "    return A2, cache\n",
        "\n",
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    #Calculating gradients\n",
        "\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "   \n",
        "    #Fill the code snippet below with gradients\n",
        "    dZ2 = A2-Y\n",
        "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
        "    dW1 = (1/m) * np.dot(dZ1, X.T) \n",
        "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    \n",
        "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2,\"db2\": db2}\n",
        "    \n",
        "    return grads\n",
        "\n",
        "def gradient_descent(parameters, grads, learning_rate = 0.1):\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "   \n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    \n",
        "    parameters = {\"W1\": W1, \"b1\": b1,\"W2\": W2,\"b2\": b2}\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "def cross_entropy_cost(A2, Y, parameters):\n",
        "    # number of training example\n",
        "    m = Y.shape[1] \n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1 - A2))\n",
        "\n",
        "    cost = - np.sum(logprobs) / m\n",
        "    cost = float(np.squeeze(cost))\n",
        "                                    \n",
        "    return cost\n",
        "\n",
        "def prediction(parameters, X):\n",
        "    A2, _ = forward_propagation(X, parameters)\n",
        "    predictions = np.round(A2)\n",
        "    \n",
        "    return predictions"
      ],
      "metadata": {
        "id": "tQ8s3HnuRe_c"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def check_accuracy(parameters): \n",
        "  predictions = prediction(parameters, X_train)\n",
        "  train_Accuracy = float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100)\n",
        "  print ('Accuracy Train: {}%'.format(train_Accuracy))\n",
        "\n",
        "  predictions = prediction(parameters, X_test)\n",
        "  test_Accuracy = float((np.dot(y_test, predictions.T) + np.dot(1 - y_test, 1 - predictions.T))/float(y_test.size)*100)\n",
        "  print ('Accuracy Test: {}%'.format(test_Accuracy))\n",
        "  return train_Accuracy,test_Accuracy"
      ],
      "metadata": {
        "id": "CarxLI0Dj0Ga"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_model(X, Y, hidden_unit, num_iterations = 2000):\n",
        "    np.random.seed(3)\n",
        "\n",
        "    \n",
        "    parameters = parameters_initialization(input_unit, hidden_unit, output_unit)\n",
        "   \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "\n",
        "    #Training the model\n",
        "    graph_x,graph_y=[],[]\n",
        "    train_acc_list,test_acc_list=[],[]\n",
        "    for i in range(0, num_iterations):\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        cost = cross_entropy_cost(A2, Y, parameters)\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        "        parameters = gradient_descent(parameters, grads)\n",
        "        if i % 50 == 0:\n",
        "            print (\"Cost after iteration {} is {}\".format(i, cost))\n",
        "            print(\"Training and Test Accuracy for this particular iteration is:\",)\n",
        "            train_acc_val,test_acc_val = check_accuracy(parameters)\n",
        "            graph_x.append(i)\n",
        "            graph_y.append(cost)\n",
        "            train_acc_list.append(train_acc_val)\n",
        "            test_acc_list.append(test_acc_val)\n",
        "\n",
        "\n",
        "    # plt.plot(graph_x, graph_y)\n",
        "    # plt.xlabel('Iteration')\n",
        "    # plt.ylabel('Cost')\n",
        "    # plt.title('Plotting Cost as Iteration Increases')\n",
        "    plt.plot(graph_x, train_acc_list)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Plotting Accuracy as Iteration Increases')\n",
        "    plt.show()       \n",
        "    return parameters\n",
        "\n",
        "parameters = neural_network_model(X_train, y_train, 4, num_iterations=2500)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lefy4UdXRptM",
        "outputId": "5a89dc39-7861-43c8-e98e-3395e3488226"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0 is 0.692936931685913\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 65.0%\n",
            "Accuracy Test: 60.56338028169014%\n",
            "Cost after iteration 50 is 0.6381791493873871\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 65.0%\n",
            "Accuracy Test: 60.56338028169014%\n",
            "Cost after iteration 100 is 0.5760092764203345\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 66.07142857142857%\n",
            "Accuracy Test: 61.97183098591549%\n",
            "Cost after iteration 150 is 0.46851183008639613\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 83.92857142857143%\n",
            "Accuracy Test: 77.46478873239437%\n",
            "Cost after iteration 200 is 0.3776432057669788\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 87.14285714285714%\n",
            "Accuracy Test: 81.69014084507043%\n",
            "Cost after iteration 250 is 0.3226686367262016\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 90.35714285714286%\n",
            "Accuracy Test: 81.69014084507043%\n",
            "Cost after iteration 300 is 0.2891828014193587\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 90.71428571428571%\n",
            "Accuracy Test: 81.69014084507043%\n",
            "Cost after iteration 350 is 0.26447959765243567\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 90.35714285714286%\n",
            "Accuracy Test: 84.50704225352112%\n",
            "Cost after iteration 400 is 0.24390552642765512\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 91.42857142857143%\n",
            "Accuracy Test: 84.50704225352112%\n",
            "Cost after iteration 450 is 0.2283382151905816\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.21428571428572%\n",
            "Accuracy Test: 84.50704225352112%\n",
            "Cost after iteration 500 is 0.21702357291709967\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.57142857142857%\n",
            "Accuracy Test: 83.09859154929578%\n",
            "Cost after iteration 550 is 0.2081345726513392\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.57142857142857%\n",
            "Accuracy Test: 83.09859154929578%\n",
            "Cost after iteration 600 is 0.2007251140405249\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.21428571428572%\n",
            "Accuracy Test: 83.09859154929578%\n",
            "Cost after iteration 650 is 0.19426433851657898\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.57142857142857%\n",
            "Accuracy Test: 83.09859154929578%\n",
            "Cost after iteration 700 is 0.18841654817028733\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.92857142857143%\n",
            "Accuracy Test: 83.09859154929578%\n",
            "Cost after iteration 750 is 0.18297798233705076\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.92857142857143%\n",
            "Accuracy Test: 83.09859154929578%\n",
            "Cost after iteration 800 is 0.17784015665851863\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.92857142857143%\n",
            "Accuracy Test: 83.09859154929578%\n",
            "Cost after iteration 850 is 0.17294924340521542\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.92857142857143%\n",
            "Accuracy Test: 83.09859154929578%\n",
            "Cost after iteration 900 is 0.16827402887118273\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.92857142857143%\n",
            "Accuracy Test: 84.50704225352112%\n",
            "Cost after iteration 950 is 0.16378869610563157\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 93.92857142857143%\n",
            "Accuracy Test: 84.50704225352112%\n",
            "Cost after iteration 1000 is 0.15946588077648707\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 94.28571428571428%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1050 is 0.15527379404744285\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 94.28571428571428%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1100 is 0.15117450256238235\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 94.28571428571428%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1150 is 0.14712278826094538\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 94.64285714285714%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1200 is 0.14306638944687172\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 95.0%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1250 is 0.13895266661685968\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 95.35714285714286%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1300 is 0.13475312358493363\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 95.35714285714286%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1350 is 0.13049869592441696\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 95.35714285714286%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1400 is 0.12626844265352835\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 95.71428571428572%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1450 is 0.12213379324385347\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 96.07142857142857%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1500 is 0.11814253730868361\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 96.78571428571429%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1550 is 0.11433023258378258\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 96.78571428571429%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1600 is 0.11072194071528367\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 96.78571428571429%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1650 is 0.10733003634866407\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.14285714285714%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1700 is 0.10415537762105223\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.14285714285714%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1750 is 0.10119060004408172\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.14285714285714%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1800 is 0.0984233332727867\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.14285714285714%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1850 is 0.09583860009344383\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.14285714285714%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1900 is 0.09342043692879147\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.14285714285714%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 1950 is 0.09115293367733092\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.5%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 2000 is 0.08902086760059688\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.5%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 2050 is 0.08701006296408882\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.5%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 2100 is 0.08510757005090548\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.5%\n",
            "Accuracy Test: 85.91549295774648%\n",
            "Cost after iteration 2150 is 0.08330172605087889\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 97.85714285714285%\n",
            "Accuracy Test: 87.32394366197182%\n",
            "Cost after iteration 2200 is 0.08158213816673802\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 98.21428571428571%\n",
            "Accuracy Test: 87.32394366197182%\n",
            "Cost after iteration 2250 is 0.07993961578182476\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 98.21428571428571%\n",
            "Accuracy Test: 87.32394366197182%\n",
            "Cost after iteration 2300 is 0.07836607096553166\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 98.21428571428571%\n",
            "Accuracy Test: 87.32394366197182%\n",
            "Cost after iteration 2350 is 0.0768544020122397\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 98.21428571428571%\n",
            "Accuracy Test: 87.32394366197182%\n",
            "Cost after iteration 2400 is 0.07539837133598641\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 98.21428571428571%\n",
            "Accuracy Test: 87.32394366197182%\n",
            "Cost after iteration 2450 is 0.07399248621386106\n",
            "Training and Test Accuracy for this particular iteration is:\n",
            "Accuracy Train: 98.21428571428571%\n",
            "Accuracy Test: 87.32394366197182%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Etj8H76RubE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_accuracy()"
      ],
      "metadata": {
        "id": "VlPWU7JhUMgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06b439e-ed99-4c9b-f7c5-9a688621adee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Train: 98.21428571428571%\n",
            "Accuracy Test: 88.73239436619718%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DhEyWw23jum_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}